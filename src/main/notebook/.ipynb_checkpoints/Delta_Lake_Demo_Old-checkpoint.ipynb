{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:26: error: value \\ is not a member of object org.apache.spark.sql.SparkSession\n       val spark = SparkSession. \\\n                                 ^\n<console>:27: error: not found: value builder\n       builder. \\\n       ^\n<console>:28: error: not found: value master\n       master(\"local\"). \\\n       ^\n<console>:29: error: not found: value appName\n       appName(\"Delta_Lake_Examples\"). \\\n       ^\n<console>:30: error: not found: value enableHiveSupport\n       enableHiveSupport(). \\\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "val spark = SparkSession. \\\n",
    "builder. \\\n",
    "master(\"local\"). \\\n",
    "appName(\"Delta_Lake_Examples\"). \\\n",
    "enableHiveSupport(). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(100).repartition(1).write.mode(\"overwrite\").csv(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [_c0: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.csv(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|_c0|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Failure(org.apache.spark.SparkException: Job aborted.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala.util.Try(\n",
    "  spark\n",
    "    .range(100)\n",
    "    .repartition(1)\n",
    "    .map { i =>\n",
    "      if (i > 50) {\n",
    "        Thread.sleep(5000)\n",
    "        throw new RuntimeException(\"Oops!\")\n",
    "      }\n",
    "      i\n",
    "    }\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "  .csv(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### http://192.168.0.104:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hist_df = [sl_no: int, gender: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sl_no: int, gender: string ... 8 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hist_df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/input/placements_hist_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- hsc_p: double (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- mba_p: integer (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|    status|salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|    1|     M| 67.0| 91.0|    58.0|   55|        Mkt&HR|    No|    Placed|270000|\n",
      "|    2|     M|79.33|78.33|   77.48|   86|       Mkt&Fin|   Yes|    Placed|200000|\n",
      "|    3|     M| 65.0| 68.0|    64.0|   75|       Mkt&Fin|    No|    Placed|250000|\n",
      "|    4|     M| 56.0| 52.0|    52.0|   66|        Mkt&HR|    No|Not Placed|  null|\n",
      "|    5|     M| 85.8| 73.6|    73.3|   96|       Mkt&Fin|    No|    Placed|425000|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df.printSchema()\n",
    "hist_df.show(5)\n",
    "hist_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.write.mode(\"overwrite\").format(\"parquet\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incr_df = [sl_no: int, gender: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sl_no: int, gender: string ... 8 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val incr_df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/input/placements_incr_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- hsc_p: integer (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- mba_p: double (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|    status|salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|  205|     F| 74.0|   73|    73.0| 80.0|       Mkt&Fin|   Yes|    Placed|210000|\n",
      "|  206|     M| 61.0|   62|    65.0| 62.0|       Mkt&Fin|    No|    Placed|250000|\n",
      "|  207|     M| 41.0|   42|    60.0| 97.0|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  208|     M|83.33|   78|    61.0| 88.0|       Mkt&Fin|   Yes|    Placed|300000|\n",
      "|  209|     F| 43.0|   60|    65.0| 92.0|        Mkt&HR|    No|Not Placed|  null|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incr_df.printSchema()\n",
    "incr_df.show(5)\n",
    "incr_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_df.write.mode(\"append\").format(\"parquet\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_df = [sl_no: int, gender: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sl_no: int, gender: string ... 8 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_df = spark.read.parquet(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- hsc_p: double (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- mba_p: integer (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.printSchema()\n",
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|    status|salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|    4|     M| 56.0| 52.0|    52.0|   66|        Mkt&HR|    No|Not Placed|  null|\n",
      "|    6|     M| 55.0| 49.8|   67.25|   55|       Mkt&Fin|   Yes|Not Placed|  null|\n",
      "|    7|     F| 46.0| 49.2|    79.0|   74|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|   10|     M| 58.0| 70.0|    61.0|   54|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|   13|     F| 47.0| 55.0|    65.0|   62|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   15|     M| 62.0| 47.0|    50.0|   76|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   18|     F| 55.0| 67.0|    64.0|   60|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|   19|     F| 63.0| 66.0|    64.0|   68|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   26|     F|52.58| 54.6|    50.2|   76|       Mkt&Fin|   Yes|Not Placed|  null|\n",
      "|   30|     M| 62.0| 67.0|    58.0|   77|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|   32|     F| 67.0| 53.0|    65.0|   64|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   35|     M| 62.0| 51.0|    52.0|   68|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   37|     M| 51.0| 44.0|    57.0|   64|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|   42|     F| 74.0|63.16|    65.0|   65|        Mkt&HR|   Yes|Not Placed|  null|\n",
      "|   43|     M| 49.0| 39.0|    65.0|   63|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|   46|     F| 76.0| 64.0|    72.0|   58|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   47|     F|70.89|71.98|    65.6|   68|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   50|     F| 50.0| 37.0|    52.0|   65|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   52|     M| 54.4|61.12|    56.2|   67|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   53|     F|40.89|45.83|    53.0|   71|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   64|     M| 61.0| 70.0|    64.0|   68|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   66|     M| 54.0| 47.0|    57.0|   89|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   69|     F| 69.7| 47.0|    72.7|   79|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   76|     F| 59.0| 62.0|    77.5|   74|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   80|     F| 69.0| 62.0|    66.0|   75|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   83|     M| 63.0| 67.0|    74.0|   82|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|   88|     M| 59.6| 51.0|    60.0|   75|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   92|     M| 52.0| 57.0|    50.8|   67|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   94|     M| 52.0| 62.0|    54.0|   72|        Mkt&HR|    No|Not Placed|  null|\n",
      "|   98|     F| 70.5| 62.5|    61.0|   93|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  100|     M| 54.0| 82.0|    63.0|   50|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  101|     F| 45.0| 57.0|    58.0|   56|        Mkt&HR|   Yes|Not Placed|  null|\n",
      "|  106|     M| 59.0| 64.0|    58.0|   85|        Mkt&HR|    No|Not Placed|  null|\n",
      "|  107|     M|61.08| 50.0|    54.0|   71|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  110|     M| 52.0| 63.0|    65.0|   86|        Mkt&HR|   Yes|Not Placed|  null|\n",
      "|  112|     M| 51.0| 54.0|    61.0|   60|        Mkt&HR|    No|Not Placed|  null|\n",
      "|  121|     M| 58.0| 40.0|    59.0|   73|        Mkt&HR|    No|Not Placed|  null|\n",
      "|  131|     M| 62.0| 65.0|    60.0|   84|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  137|     F| 47.0| 59.0|    64.0|   78|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  142|     M| 66.0| 64.0|    60.0|   60|        Mkt&HR|    No|Not Placed|  null|\n",
      "|  145|     M| 52.0| 50.0|    61.0|   60|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  150|     M| 44.0| 58.0|    55.0|   64|        Mkt&HR|   Yes|Not Placed|  null|\n",
      "|  156|     M|51.57|74.66|    59.9|   56|        Mkt&HR|   Yes|Not Placed|  null|\n",
      "|  159|     M| 67.0| 63.0|    64.0|   60|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  160|     M| 52.0| 49.0|    58.0|   62|        Mkt&HR|    No|Not Placed|  null|\n",
      "|  162|     M| 55.6| 51.0|    57.5|   57|        Mkt&HR|    No|Not Placed|  null|\n",
      "|  166|     F| 63.3|78.33|    74.0|   80|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  168|     M| 67.9| 62.0|    67.0|   58|       Mkt&Fin|   Yes|Not Placed|  null|\n",
      "|  169|     F| 48.0| 51.0|    58.0|   60|        Mkt&HR|   Yes|Not Placed|  null|\n",
      "|  170|     M|59.96|42.16|   61.26|   54|        Mkt&HR|    No|Not Placed|  null|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.filter(final_df(\"status\")===\"Not Placed\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 42, localhost, executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/part-00000-f61f5ad6-3ebc-4e40-86e3-9104a8e13c61-c000.snappy.parquet. Column: [hsc_p], Expected: double, Found: INT32\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:250)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readIntBatch(VectorizedColumnReader.java:420)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:205)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n\t... 23 more\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 42, localhost, executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/part-00000-f61f5ad6-3ebc-4e40-86e3-9104a8e13c61-c000.snappy.parquet. Column: [hsc_p], Expected: double, Found: INT32",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:250)",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readIntBatch(VectorizedColumnReader.java:420)",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:205)",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)",
      "\t... 23 more",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)",
      "  ... 42 elided",
      "Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/part-00000-f61f5ad6-3ebc-4e40-86e3-9104a8e13c61-c000.snappy.parquet. Column: [hsc_p], Expected: double, Found: INT32",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  ... 3 more",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException",
      "  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:250)",
      "  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readIntBatch(VectorizedColumnReader.java:420)",
      "  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:205)",
      "  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)",
      "  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)",
      "  at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)",
      "  ... 23 more"
     ]
    }
   ],
   "source": [
    "final_df.filter(final_df(\"status\")===\"Not Placed\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column: [hsc_p], Expected: double, Found: INT32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.range(100).select($\"id\".as(\"id\")).repartition(1).write.mode(\"overwrite\").format(\"delta\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"delta\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Failure(org.apache.spark.SparkException: Job aborted.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala.util.Try(spark.range(100).repartition(1).map { i =>\n",
    "      if (i > 50) {\n",
    "        Thread.sleep(5000)\n",
    "        throw new RuntimeException(\"Oops!\")\n",
    "      }\n",
    "    i\n",
    "  }.select($\"value\".as(\"id\")).write.mode(\"overwrite\").format(\"delta\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"delta\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Success(())"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala.util.Try(spark.range(50).repartition(1).map { i =>\n",
    "      if (i > 100) {\n",
    "        Thread.sleep(5000)\n",
    "        throw new RuntimeException(\"Oops!\")\n",
    "      }\n",
    "    i\n",
    "  }.select($\"value\".as(\"id\")).write.mode(\"overwrite\").format(\"delta\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"delta\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hist_df = [sl_no: int, gender: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sl_no: int, gender: string ... 8 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hist_df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/input/placements_hist_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- hsc_p: double (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- mba_p: integer (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|status    |salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|1    |M     |67.0 |91.0 |58.0    |55   |Mkt&HR        |No    |Placed    |270000|\n",
      "|2    |M     |79.33|78.33|77.48   |86   |Mkt&Fin       |Yes   |Placed    |200000|\n",
      "|3    |M     |65.0 |68.0 |64.0    |75   |Mkt&Fin       |No    |Placed    |250000|\n",
      "|4    |M     |56.0 |52.0 |52.0    |66   |Mkt&HR        |No    |Not Placed|null  |\n",
      "|5    |M     |85.8 |73.6 |73.3    |96   |Mkt&Fin       |No    |Placed    |425000|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df.printSchema()\n",
    "hist_df.show(5, false)\n",
    "hist_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.write.format(\"delta\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incr_df = [sl_no: int, gender: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sl_no: int, gender: string ... 8 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val incr_df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/input/placements_incr_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- hsc_p: integer (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- mba_p: double (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|    status|salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|  205|     F| 74.0|   73|    73.0| 80.0|       Mkt&Fin|   Yes|    Placed|210000|\n",
      "|  206|     M| 61.0|   62|    65.0| 62.0|       Mkt&Fin|    No|    Placed|250000|\n",
      "|  207|     M| 41.0|   42|    60.0| 97.0|       Mkt&Fin|    No|Not Placed|  null|\n",
      "|  208|     M|83.33|   78|    61.0| 88.0|       Mkt&Fin|   Yes|    Placed|300000|\n",
      "|  209|     F| 43.0|   60|    65.0| 92.0|        Mkt&HR|    No|Not Placed|  null|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incr_df.printSchema()\n",
    "incr_df.show(5)\n",
    "incr_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "Failed to merge fields 'hsc_p' and 'hsc_p'. Failed to merge incompatible data types DoubleType and IntegerType;;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Failed to merge fields 'hsc_p' and 'hsc_p'. Failed to merge incompatible data types DoubleType and IntegerType;;",
      "  at org.apache.spark.sql.delta.schema.SchemaUtils$$anonfun$15.apply(SchemaUtils.scala:526)",
      "  at org.apache.spark.sql.delta.schema.SchemaUtils$$anonfun$15.apply(SchemaUtils.scala:515)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at org.apache.spark.sql.delta.schema.SchemaUtils$.org$apache$spark$sql$delta$schema$SchemaUtils$$merge$1(SchemaUtils.scala:515)",
      "  at org.apache.spark.sql.delta.schema.SchemaUtils$.mergeSchemas(SchemaUtils.scala:591)",
      "  at org.apache.spark.sql.delta.schema.ImplicitMetadataOperation$class.updateMetadata(ImplicitMetadataOperation.scala:60)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:45)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:84)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:65)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:64)",
      "  at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:386)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:64)",
      "  at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:131)",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)",
      "  ... 42 elided"
     ]
    }
   ],
   "source": [
    "incr_df.write.mode(\"append\").format(\"delta\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- hsc_p: double (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- mba_p: integer (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "final_df = [sl_no: int, gender: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_df = spark.read.format(\"delta\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")\n",
    "final_df.printSchema()\n",
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io.delta.tables._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt = io.delta.tables.DeltaTable@65e98b48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "io.delta.tables.DeltaTable@65e98b48"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dt = DeltaTable.forPath(spark, \"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|status    |salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "|1    |M     |67.0 |91.0 |58.0    |55   |Mkt&HR        |No    |Placed    |270000|\n",
      "|2    |M     |79.33|78.33|77.48   |86   |Mkt&Fin       |Yes   |Placed    |200000|\n",
      "|3    |M     |65.0 |68.0 |64.0    |75   |Mkt&Fin       |No    |Placed    |250000|\n",
      "|4    |M     |56.0 |52.0 |52.0    |66   |Mkt&HR        |No    |Not Placed|null  |\n",
      "|5    |M     |85.8 |73.6 |73.3    |96   |Mkt&Fin       |No    |Placed    |425000|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.toDF.show(5, false)\n",
    "dt.toDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.delete(\"status == 'Not Placed'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|status|salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "|1    |M     |67.0 |91.0 |58.0    |55   |Mkt&HR        |No    |Placed|270000|\n",
      "|2    |M     |79.33|78.33|77.48   |86   |Mkt&Fin       |Yes   |Placed|200000|\n",
      "|3    |M     |65.0 |68.0 |64.0    |75   |Mkt&Fin       |No    |Placed|250000|\n",
      "|5    |M     |85.8 |73.6 |73.3    |96   |Mkt&Fin       |No    |Placed|425000|\n",
      "|8    |M     |82.0 |64.0 |66.0    |67   |Mkt&Fin       |Yes   |Placed|252000|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.toDF.show(5, false)\n",
    "dt.toDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.updateExpr(\"specialisation == 'Mkt&HR'\", Map(\"salary\" -> \"salary + 50000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|status|salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "|1    |M     |67.0 |91.0 |58.0    |55   |Mkt&HR        |No    |Placed|320000|\n",
      "|2    |M     |79.33|78.33|77.48   |86   |Mkt&Fin       |Yes   |Placed|200000|\n",
      "|3    |M     |65.0 |68.0 |64.0    |75   |Mkt&Fin       |No    |Placed|250000|\n",
      "|5    |M     |85.8 |73.6 |73.3    |96   |Mkt&Fin       |No    |Placed|425000|\n",
      "|8    |M     |82.0 |64.0 |66.0    |67   |Mkt&Fin       |Yes   |Placed|252000|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF.show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hist_df = [IATA: string, AIRPORT: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[IATA: string, AIRPORT: string ... 3 more fields]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hist_df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/input/iata_hist_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA: string (nullable = true)\n",
      " |-- AIRPORT: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      "\n",
      "+----+-----------------------------------+-----------+-----+-------+\n",
      "|IATA|AIRPORT                            |CITY       |STATE|COUNTRY|\n",
      "+----+-----------------------------------+-----------+-----+-------+\n",
      "|AKN |King Salmon                        |King Salmon|AK   |USA    |\n",
      "|ABQ |Albuquerque International          |Albuquerque|NM   |USA    |\n",
      "|ANC |Ted Stevens Anchorage International|Anchorage  |AK   |USA    |\n",
      "|ATL |William B Hartsfield-Atlanta Intl  |Atlanta    |GA   |USA    |\n",
      "|AUS |Austin-Bergstrom International     |Austin     |TX   |USA    |\n",
      "+----+-----------------------------------+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df.printSchema()\n",
    "hist_df.show(5, false)\n",
    "hist_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.write.format(\"delta\").mode(\"overwrite\").save(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------------+-----------+-----+-------+\n",
      "|IATA|AIRPORT                            |CITY       |STATE|COUNTRY|\n",
      "+----+-----------------------------------+-----------+-----+-------+\n",
      "|AKN |King Salmon                        |King Salmon|AK   |USA    |\n",
      "|ABQ |Albuquerque International          |Albuquerque|NM   |USA    |\n",
      "|ANC |Ted Stevens Anchorage International|Anchorage  |AK   |USA    |\n",
      "|ATL |William B Hartsfield-Atlanta Intl  |Atlanta    |GA   |USA    |\n",
      "|AUS |Austin-Bergstrom International     |Austin     |TX   |USA    |\n",
      "+----+-----------------------------------+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt = io.delta.tables.DeltaTable@71f9094\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "io.delta.tables.DeltaTable@71f9094"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dt = DeltaTable.forPath(spark, \"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e3/\")\n",
    "dt.toDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incr_df = [IATA: string, AIRPORT: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[IATA: string, AIRPORT: string ... 3 more fields]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val incr_df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/input/iata_incr_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA: string (nullable = true)\n",
      " |-- AIRPORT: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      "\n",
      "+----+-------------------+-------------+-----+-------+\n",
      "|IATA|AIRPORT            |CITY         |STATE|COUNTRY|\n",
      "+----+-------------------+-------------+-----+-------+\n",
      "|AKN |King Salmon Updated|King Salmon  |AK   |USA    |\n",
      "|BFL |Meadows New        |Bakersfield  |CA   |USA    |\n",
      "|STX |Henry E. Rohlsen   |Christiansted|VI   |USA    |\n",
      "|SUN |Friedman Memorial  |Hailey       |ID   |USA    |\n",
      "|SUX |Sioux Gateway      |Sioux City   |IA   |USA    |\n",
      "+----+-------------------+-------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incr_df.printSchema()\n",
    "incr_df.show(5, false)\n",
    "incr_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+-----+-------+\n",
      "|IATA| AIRPORT|       CITY|STATE|COUNTRY|\n",
      "+----+--------+-----------+-----+-------+\n",
      "| BFL|Meadows |Bakersfield|   CA|    USA|\n",
      "+----+--------+-----------+-----+-------+\n",
      "\n",
      "+----+-----------+-----------+-----+-------+\n",
      "|IATA|    AIRPORT|       CITY|STATE|COUNTRY|\n",
      "+----+-----------+-----------+-----+-------+\n",
      "| BFL|Meadows New|Bakersfield|   CA|    USA|\n",
      "+----+-----------+-----------+-----+-------+\n",
      "\n",
      "+----+-----------+-----------+-----+-------+\n",
      "|IATA|    AIRPORT|       CITY|STATE|COUNTRY|\n",
      "+----+-----------+-----------+-----+-------+\n",
      "| AKN|King Salmon|King Salmon|   AK|    USA|\n",
      "+----+-----------+-----------+-----+-------+\n",
      "\n",
      "+----+-------------------+-----------+-----+-------+\n",
      "|IATA|            AIRPORT|       CITY|STATE|COUNTRY|\n",
      "+----+-------------------+-----------+-----+-------+\n",
      "| AKN|King Salmon Updated|King Salmon|   AK|    USA|\n",
      "+----+-------------------+-----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_df.filter(hist_df(\"IATA\") === \"BFL\").show()\n",
    "incr_df.filter(incr_df(\"IATA\") === \"BFL\").show()\n",
    "hist_df.filter(hist_df(\"IATA\") === \"AKN\").show()\n",
    "incr_df.filter(incr_df(\"IATA\") === \"AKN\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.as(\"hist\").merge(incr_df.as(\"incr\"), \"hist.IATA = incr.IATA\").whenMatched().updateExpr(Map(\"AIRPORT\" -> \"incr.AIRPORT\", \"CITY\" -> \"incr.CITY\", \"STATE\" -> \"incr.STATE\", \"COUNTRY\" -> \"incr.COUNTRY\")).whenNotMatched.insertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_df = [IATA: string, AIRPORT: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[IATA: string, AIRPORT: string ... 3 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_df = spark.read.format(\"delta\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----------+-----+-------+\n",
      "|IATA|            AIRPORT|       CITY|STATE|COUNTRY|\n",
      "+----+-------------------+-----------+-----+-------+\n",
      "| AKN|King Salmon Updated|King Salmon|   AK|    USA|\n",
      "+----+-------------------+-----------+-----+-------+\n",
      "\n",
      "+----+-----------+-----------+-----+-------+\n",
      "|IATA|    AIRPORT|       CITY|STATE|COUNTRY|\n",
      "+----+-----------+-----------+-----+-------+\n",
      "| BFL|Meadows New|Bakersfield|   CA|    USA|\n",
      "+----+-----------+-----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.filter(final_df(\"IATA\") === \"AKN\" ).show()\n",
    "final_df.filter(final_df(\"IATA\") === \"BFL\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt = io.delta.tables.DeltaTable@43065d62\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "io.delta.tables.DeltaTable@43065d62"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dt = DeltaTable.forPath(spark, \"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+---------------------------------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|version|timestamp          |userId|userName|operation|operationParameters                          |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|\n",
      "+-------+-------------------+------+--------+---------+---------------------------------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|2      |2020-08-20 17:21:39|null  |null    |UPDATE   |[predicate -> (specialisation#2488 = Mkt&HR)]|null|null    |null     |1          |null          |false        |\n",
      "|1      |2020-08-20 17:21:27|null  |null    |DELETE   |[predicate -> [\"(`status` = 'Not Placed')\"]] |null|null    |null     |0          |null          |false        |\n",
      "|0      |2020-08-20 17:21:13|null  |null    |WRITE    |[mode -> ErrorIfExists, partitionBy -> []]   |null|null    |null     |null       |null          |true         |\n",
      "+-------+-------------------+------+--------+---------+---------------------------------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.history().show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "|sl_no|gender|ssc_p|hsc_p|degree_p|mba_p|specialisation|workex|status|salary|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "|    1|     M| 67.0| 91.0|    58.0|   55|        Mkt&HR|    No|Placed|320000|\n",
      "|    2|     M|79.33|78.33|   77.48|   86|       Mkt&Fin|   Yes|Placed|200000|\n",
      "|    3|     M| 65.0| 68.0|    64.0|   75|       Mkt&Fin|    No|Placed|250000|\n",
      "|    5|     M| 85.8| 73.6|    73.3|   96|       Mkt&Fin|    No|Placed|425000|\n",
      "|    8|     M| 82.0| 64.0|    66.0|   67|       Mkt&Fin|   Yes|Placed|252000|\n",
      "|    9|     M| 73.0| 79.0|    72.0|   91|       Mkt&Fin|    No|Placed|231000|\n",
      "|   11|     M| 58.0| 61.0|    60.0|   62|        Mkt&HR|   Yes|Placed|310000|\n",
      "|   12|     M| 69.6| 68.4|    78.3|   60|       Mkt&Fin|   Yes|Placed|250000|\n",
      "|   14|     F| 77.0| 87.0|    59.0|   68|       Mkt&Fin|    No|Placed|218000|\n",
      "|   16|     F| 65.0| 75.0|    69.0|   72|       Mkt&Fin|   Yes|Placed|200000|\n",
      "|   17|     M| 63.0| 66.2|    65.6|   60|       Mkt&Fin|   Yes|Placed|300000|\n",
      "|   20|     M| 60.0| 67.0|    70.0|   50|       Mkt&Fin|   Yes|Placed|236000|\n",
      "|   21|     M| 62.0| 65.0|    66.0|   50|        Mkt&HR|    No|Placed|315000|\n",
      "|   22|     F| 79.0| 76.0|    85.0|   95|       Mkt&Fin|    No|Placed|393000|\n",
      "|   23|     F| 69.8| 60.8|   72.23|   55|        Mkt&HR|    No|Placed|410000|\n",
      "|   24|     F| 77.4| 60.0|   64.74|   92|       Mkt&Fin|   Yes|Placed|300000|\n",
      "|   25|     M| 76.5| 97.7|   78.86|   97|       Mkt&Fin|    No|Placed|360000|\n",
      "|   27|     M| 71.0| 79.0|    66.0|   94|       Mkt&Fin|   Yes|Placed|240000|\n",
      "|   28|     M| 63.0| 67.0|    66.0|   68|        Mkt&HR|    No|Placed|315000|\n",
      "|   29|     M|76.76| 76.5|    67.5|   73|       Mkt&Fin|   Yes|Placed|350000|\n",
      "+-----+------+-----+-----+--------+-----+--------------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\",2).load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+---------------------------------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|version|timestamp          |userId|userName|operation|operationParameters                          |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|\n",
      "+-------+-------------------+------+--------+---------+---------------------------------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|2      |2020-08-20 17:21:39|null  |null    |UPDATE   |[predicate -> (specialisation#2488 = Mkt&HR)]|null|null    |null     |1          |null          |false        |\n",
      "|1      |2020-08-20 17:21:27|null  |null    |DELETE   |[predicate -> [\"(`status` = 'Not Placed')\"]] |null|null    |null     |0          |null          |false        |\n",
      "|0      |2020-08-20 17:21:13|null  |null    |WRITE    |[mode -> ErrorIfExists, partitionBy -> []]   |null|null    |null     |null       |null          |true         |\n",
      "+-------+-------------------+------+--------+---------+---------------------------------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.history().show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "The provided timestamp (2020-08-20 16:55:21.0) is before the earliest version available to this\ntable (2020-08-20 17:21:13.0). Please use a timestamp after 2020-08-20 17:21:13.\n         ;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: The provided timestamp (2020-08-20 16:55:21.0) is before the earliest version available to this",
      "table (2020-08-20 17:21:13.0). Please use a timestamp after 2020-08-20 17:21:13.",
      "         ;",
      "  at org.apache.spark.sql.delta.DeltaErrors$.timestampEarlierThanCommitRetention(DeltaErrors.scala:501)",
      "  at org.apache.spark.sql.delta.DeltaHistoryManager.getActiveCommitAtTime(DeltaHistoryManager.scala:132)",
      "  at org.apache.spark.sql.delta.DeltaTableUtils$.resolveTimeTravelVersion(DeltaTable.scala:226)",
      "  at org.apache.spark.sql.delta.DeltaLog$$anonfun$31.apply(DeltaLog.scala:607)",
      "  at org.apache.spark.sql.delta.DeltaLog$$anonfun$31.apply(DeltaLog.scala:606)",
      "  at scala.Option.map(Option.scala:146)",
      "  at org.apache.spark.sql.delta.DeltaLog.createRelation(DeltaLog.scala:606)",
      "  at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:205)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)",
      "  ... 44 elided"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"timestampAsOf\", \"2020-08-20 16:55:21\").load(\"file:///home/rameshbabug/Documents/projects/internal/delta-lake-examples/src/main/data/output/e2/\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
